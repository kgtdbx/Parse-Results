{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swingbench Benchmark Run Comparison\n",
    "The following notebook takes a number of results files generated by swingbench and analyses their results side by side. This is particulalrly useful if you are changing a specific parameter between runs i.e. cpu count, user count, sga size etc.\n",
    "\n",
    "### Dependencies\n",
    "This code requires Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import humanize\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from IPython.display import HTML, display, Markdown\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "# get cpu count from file name. format = \"results_scale<val>_uc<val>_cpu<val>.xml\"\n",
    "cpu_from_string = lambda s : re.findall('(cpu)([0-9]*)',s)[0][1]\n",
    "user_count_from_tring = lambda s : re.findall('(uc)([0-9]*)',s)[0][1]\n",
    "scale_from_string = lambda s : int(re.findall('(scale)([0-9]*)',s)[0][1])\n",
    "\n",
    "def get_namespace(element:ET.Element) -> str:\n",
    "    m = re.match('\\{.*\\}', element.tag)\n",
    "    return m.group(0) if m else ''\n",
    "\n",
    "def get_key_value(element:ET.Element, section:str, key:str) -> str:\n",
    "    try:\n",
    "        return element.find('.//{0}{1}/{0}{2}'.format(namespace, section, key)).text\n",
    "    except:\n",
    "        print(f\"ERROR : Can't find {section}/{key}\")\n",
    "\n",
    "def get_tx_results(doc:ET.Element, namespace:str, tx_type:str, tx_id:str) -> str:\n",
    "    attrib = doc.find(\".//*{0}Result[@id='{1}']/{0}{2}\".format(namespace, tx_id, tx_type))\n",
    "    if attrib is not None:\n",
    "        return attrib.text\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Change the parameters below to reflect your environment. I typically have results files that are names based on the scale, usercount and allocated CPUs.\n",
    "* The first parameter `file_directory` is the directory that your results files are located in.\n",
    "* The second parameter `file_wild_card` is a wild card (regular expression) string that matches files in the `file_directory`\n",
    "* The third parameter `sort_function` is a python function that extracts a dimension from the filename that you want data sorted on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = 'dbcs_tests'\n",
    "file_wild_card = '*scale[0-9]*_uc64*cpu8.xml'\n",
    "sort_function = scale_from_string\n",
    "sort_term = \"Scale of Database\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfiles = glob.glob(os.path.join(file_directory,file_wild_card))\n",
    "\n",
    "xmldocs = {}\n",
    "\n",
    "for fileToParse in xmlfiles:\n",
    "    tree = ET.parse(fileToParse)\n",
    "    root = tree.getroot()\n",
    "    namespace = get_namespace(root)\n",
    "    xmldocs[os.path.basename(fileToParse)] = root\n",
    "    \n",
    "sortedxmldocs = OrderedDict(sorted(xmldocs.items(), key=lambda e : sort_function(e[0])))\n",
    "\n",
    "file_df = pd.DataFrame({\"Path\":[os.path.abspath(f) for f in xmlfiles],\n",
    "                        \"Size\":[humanize.naturalsize(os.path.getsize(f)) for f in xmlfiles],\n",
    "                        \"Created\":[time.ctime(os.path.getctime(f)) for f in xmlfiles]\n",
    "                       })\n",
    "display(Markdown('**Processing the following files**'))\n",
    "display(file_df)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Overview Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first doc and use it to get transactions and metric types\n",
    "first_doc = list(sortedxmldocs.items())[0][1]\n",
    "\n",
    "# Get the tags for the Overview Section\n",
    "val:[ET.Element] = first_doc.findall(f\".//{namespace}Overview/*\")\n",
    "overview_tags:[str] = [t.tag.split('}', 1)[1] for t in val]\n",
    "\n",
    "# Format the results so they look reasonable    \n",
    "table_values = {}\n",
    "table_values[\"Description\"] = overview_tags\n",
    "for file_name, doc in sortedxmldocs.items():\n",
    "    table_values[file_name] = [get_key_value(doc, 'Overview', key) for key in overview_tags]\n",
    "overview_df = pd.DataFrame(table_values)\n",
    "overview_df.set_index(['Description'],inplace=True)\n",
    "display(overview_df)\n",
    "\n",
    "# Plot the results of the tables\n",
    "# cpus = [int(cpu_from_string(s)) for s in overview_df.loc['AverageTransactionsPerSecond'].index]\n",
    "\n",
    "measures = [int(scale_from_string(s)) for s in overview_df.loc['AverageTransactionsPerSecond'].index]\n",
    "\n",
    "average_TPS = overview_df.loc['AverageTransactionsPerSecond'].astype('float').values\n",
    "fig=plt.figure(figsize=(22, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(x=measures,height=average_TPS,color='r',alpha=0.4)\n",
    "plt.xlabel(sort_term)\n",
    "plt.ylabel(\"TPS\")\n",
    "plt.title(f\"Transactions Per Second against {sort_term}\")\n",
    "plt.subplot(1, 3, 2)\n",
    "failed_tx = overview_df.loc['MaximumTransactionRate'].astype('int32').values\n",
    "plt.bar(x=measures,height=failed_tx,color='g',alpha=0.4)\n",
    "plt.xlabel(sort_term)\n",
    "plt.ylabel(\"Maximum Transaction Rate TPM\")\n",
    "plt.title(f\"Max Transaction Rate (TPM) against {sort_term}\")\n",
    "plt.subplot(1, 3, 3)\n",
    "failed_tx = overview_df.loc['TotalFailedTransactions'].astype('int32').values\n",
    "plt.bar(x=measures,height=failed_tx,color='b',alpha=0.4)\n",
    "plt.xlabel(sort_term)\n",
    "plt.ylabel(\"Failed Transactions\")\n",
    "ax = plt.title(f\"Failed Transactions against {sort_term}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of all Configuration Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tags for the Overview Section\n",
    "val:[ET.Element] = first_doc.findall(f\".//{namespace}Configuration/*\")\n",
    "configuration_tags:[str] = [t.tag.split('}', 1)[1] for t in val]\n",
    "\n",
    "table_values = {}\n",
    "table_values[\"Description\"] = configuration_tags\n",
    "for file_name, doc in sortedxmldocs.items():\n",
    "    table_values[file_name] = [get_key_value(doc, 'Configuration', key) for key in configuration_tags]\n",
    "config_df = pd.DataFrame(table_values)\n",
    "config_df.set_index(['Description'],inplace=True)\n",
    "config_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DML Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tags for the Overview Section\n",
    "val:[ET.Element] = first_doc.findall(f\".//{namespace}DMLResults/*\")\n",
    "dml_tags:[str] = [t.tag.split('}', 1)[1] for t in val]\n",
    "\n",
    "table_values = {}\n",
    "table_values[\"Description\"] = dml_tags\n",
    "for file_name, doc in sortedxmldocs.items():\n",
    "    table_values[file_name] = [int(get_key_value(doc, 'DMLResults', key)) for key in dml_tags]\n",
    "dml_df = pd.DataFrame(table_values)\n",
    "dml_df.set_index(['Description'],inplace=True)\n",
    "display(dml_df)\n",
    "\n",
    "# Needs to change this to DML/Operations per sec\n",
    "width = 0.15\n",
    "index = np.arange(len(measures))\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(22)\n",
    "fig.set_figheight(8)\n",
    "for offset, dml_type in enumerate(dml_tags):\n",
    "    vals = dml_df.loc[dml_type].astype('int32').values\n",
    "    rect = ax.bar(index+(width*offset), vals, width, alpha=0.7, label=dml_type)\n",
    "ax.set_xticks(index + width / 0.5)\n",
    "ax.set_xlabel(sort_term)\n",
    "ax.set_ylabel(\"DML Operations\")\n",
    "ax.set_title(f\"Total DML Operations against {sort_term}\")\n",
    "ax.legend()\n",
    "labels = ax.set_xticklabels(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of tuples containing the transaction id and result\n",
    "tx_results:Tuple[str,ET.Element] = [(tx_result.attrib.get('id'),tx_result) for tx_result in first_doc.findall(f'.//{namespace}Result')]\n",
    "# Get the names of the metrics\n",
    "val:[ET.Element] = first_doc.findall(f\".//*{namespace}Result[@id='{tx_results[0][0]}']/*\")\n",
    "metric_tags:[str] = [t.tag.split('}', 1)[1] for t in val]\n",
    "\n",
    "\n",
    "df_array = []\n",
    "for tx_name, tx_result in tx_results:\n",
    "    file_results = {}\n",
    "    for file_name, doc in sortedxmldocs.items():\n",
    "        file_results[file_name] = [get_tx_results(doc, namespace, m , tx_name) for m in metric_tags]\n",
    "    tx_frame = pd.DataFrame(file_results)\n",
    "    tx_frame[\"Result\"] = metric_tags\n",
    "    tx_frame[\"Transaction_id\"] = tx_name\n",
    "    df_array.append(tx_frame)\n",
    "df = pd.concat(df_array)\n",
    "df.set_index(['Transaction_id','Result'], inplace=True)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "display(df)\n",
    "\n",
    "width = 0.15\n",
    "index = np.arange(len(measures))\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(22)\n",
    "fig.set_figheight(8)\n",
    "for offset,tx_id in enumerate(tx_results):\n",
    "    vals = df.loc[(tx_id[0],'AverageResponse')].astype('float').values\n",
    "    rect = ax.bar(index+(width*offset), vals, width, alpha=0.7, label=tx_id[0])\n",
    "ax.set_xticks(index + width / 0.5)\n",
    "ax.set_xlabel(sort_term)\n",
    "ax.set_ylabel(\"Response Time\")\n",
    "ax.set_title(f\"Transaction Response Time against {sort_term}\")\n",
    "ax.legend()\n",
    "labels = ax.set_xticklabels(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Wait Events\n",
    "If you've enables database statistics collections the following section will display the wait events for each run and a series of charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data on wait events and print them out in a table\n",
    "wait_events = {}\n",
    "for file_name, doc in sortedxmldocs.items():\n",
    "    wait_event:Tuple[str,ET.Element] = [(tx_result.attrib.get('name'),float(tx_result.attrib.get('percentageTimeWaited'))) for tx_result in doc.findall(f'.//{namespace}DatabaseWaitEvent')]\n",
    "    res = list(zip(*wait_event)) \n",
    "    wait_events[f'{file_name}_event'] = res[0]\n",
    "    wait_events[f'{file_name}_value'] = res[1]\n",
    "\n",
    "wait_df = pd.DataFrame(wait_events)\n",
    "display(wait_df)\n",
    "\n",
    "\n",
    "# Try and get all of the unique strings for the wait events.\n",
    "event_name_cols = [col for col in wait_df.columns if 'event' in col]\n",
    "all_events = pd.concat(pd.Series(wait_df[s]) for s in event_name_cols).unique()\n",
    "\n",
    "# Map a colour map to the unique number of events\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = cmap(np.linspace(0, 1, len(all_events)))\n",
    "\n",
    "# Map Event names to the colours we've created\n",
    "persisted_map = {}\n",
    "for c in zip(all_events, colors):\n",
    "    persisted_map[c[0]] = c[1]\n",
    "\n",
    "    \n",
    "# Plot the wait events\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(22)\n",
    "fig.set_figheight(12)\n",
    "counter = 1\n",
    "for file_name, doc in sortedxmldocs.items():\n",
    "    # Create a color array mapping names to colours\n",
    "    event_colours = [persisted_map[x] for x in wait_df[f'{file_name}_event']]\n",
    "    ax = plt.subplot(3, 3, counter)\n",
    "    plt.barh(y=wait_df[f'{file_name}_event'],width=wait_df[f'{file_name}_value'],alpha=0.8,color=event_colours)\n",
    "    ax.set_title(f'{sort_term} : {sort_function(file_name)}')\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    counter += 1\n",
    "                                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Statistics\n",
    "If database statisitics are collected then they'll be displayed below. It's likely that you won't get the same number of stats back from the database for each run so there maybe a few discprencies when comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_stats = {}\n",
    "for file_name, doc in sortedxmldocs.items():\n",
    "    db_stat:Tuple[str,ET.Element] = [(dbs.attrib.get('name'),dbs.attrib.get('value')) for dbs in doc.findall(f'.//{namespace}DatabaseStatistic')]\n",
    "    db_stats[f'{file_name}_name'], db_stats[f'{file_name}_value'] = list(zip(*db_stat))\n",
    "\n",
    "# Join all the results from the file togther knowing that theymight not all have the same stats\n",
    "dbs_df = pd.concat([pd.Series(v, name=k) for k, v in db_stats.items()], axis=1)\n",
    "dbs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
